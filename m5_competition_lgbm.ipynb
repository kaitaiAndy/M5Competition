{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 Forecasting\n",
    "\n",
    "[Introduction](#Introduction)\n",
    "\n",
    "\n",
    "[EDA](#EDA)\n",
    "\n",
    "To-do\n",
    "- Denoising\n",
    "\n",
    "Statistical Model\n",
    "- ARIMA\n",
    "- Exponential Smoothing\n",
    "- Theta Method\n",
    "\n",
    "Machine Learning Model\n",
    "- GBM\n",
    "- LSTM\n",
    "Multi-step ahead forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Goal\n",
    "Predict Sales data provided by Walmart **28** days into the future\n",
    "\n",
    "## Data\n",
    "sales_train.csv: this is our main training data. It has 1 column for each of the 1941 days from 2011-01-29 and 2016-05-22; not including the validation period of 28 days until 2016-06-19. It also includes the IDs for item, department, category, store, and state. The number of rows is 30490 for all combinations of 30490 items and 10 stores.\n",
    "\n",
    "sell_prices.csv: the store and item IDs together with the sales price of the item as a weekly average.\n",
    "\n",
    "calendar.csv: dates together with related features like day-of-the week, month, year, and an 3 binary flags for whether the stores in each state allowed purchases with SNAP food stamps at this date (1) or not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import joblib\n",
    "import sys\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = {'id': object, 'day': object, 'demand': np.int16, 'weekday': object, 'wday': np.int8,\n",
    "        'month': np.int8, 'year': np.int16, 'snap_CA': np.int8, 'snap_TX': np.int8, \n",
    "        'snap_WI': np.int8, 'sell_price': np.float16, 'lag_1': np.float16, 'lag_2': np.float16,\n",
    "        'lag_3': np.float16, 'lag_4': np.float16, 'lag_5': np.float16, 'lag_6': np.float16, \n",
    "        'lag_7': np.float16, 'lag_28': np.float16, 'rmean_7_7': np.float16, 'rmean_28_7': np.float16,\n",
    "        'rmean_7_28': np.float16, 'rmean_28_28': np.float16, 'item_id': np.int64, 'dept_id': np.int64,\n",
    "        'store_id': np.int64, 'cat_id': np.int64, 'state_id': np.int64, 'event_name_1': np.int64,\n",
    "        'event_name_2': np.int64, 'event_type_1': np.int64, 'event_type_2': np.int64}\n",
    "\n",
    "FEATURE = ['wday', 'month', 'year', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price',\n",
    "           'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'lag_28',\n",
    "           'rmean_7_7', 'rmean_28_7', 'rmean_7_28', 'rmean_28_28', 'item_id',\n",
    "           'dept_id', 'store_id', 'cat_id', 'state_id', 'event_name_1',\n",
    "           'event_name_2', 'event_type_1', 'event_type_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "util function\n",
    "https://www.kaggle.com/ratan123/m5-forecasting-lightgbm-with-timeseries-splits\n",
    "'''\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_name, sales_path='sales_train_validation.csv',\n",
    "             calendar_path='calendar.csv', price_path='sell_prices.csv'):\n",
    "    sales = pd.read_csv(os.path.join(dir_name, sales_path))\n",
    "    calendar = pd.read_csv(os.path.join(dir_name, calendar_path))\n",
    "    price = pd.read_csv(os.path.join(dir_name, price_path))\n",
    "    return sales, calendar, price\n",
    "#     return reduce_mem_usage(sales), reduce_mem_usage(calendar), reduce_mem_usage(price)\n",
    "\n",
    "sales, calendar, price = load_data('../m5-forecasting-accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1904</th>\n",
       "      <th>d_1905</th>\n",
       "      <th>d_1906</th>\n",
       "      <th>d_1907</th>\n",
       "      <th>d_1908</th>\n",
       "      <th>d_1909</th>\n",
       "      <th>d_1910</th>\n",
       "      <th>d_1911</th>\n",
       "      <th>d_1912</th>\n",
       "      <th>d_1913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1904  d_1905  d_1906  d_1907  d_1908  \\\n",
       "0       CA    0    0    0    0  ...       1       3       0       1       1   \n",
       "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
       "2       CA    0    0    0    0  ...       2       1       2       1       1   \n",
       "3       CA    0    0    0    0  ...       1       0       5       4       1   \n",
       "4       CA    0    0    0    0  ...       2       1       1       0       1   \n",
       "\n",
       "   d_1909  d_1910  d_1911  d_1912  d_1913  \n",
       "0       1       3       0       1       1  \n",
       "1       1       0       0       0       0  \n",
       "2       1       0       1       1       1  \n",
       "3       0       1       3       7       2  \n",
       "4       1       2       2       2       4  \n",
       "\n",
       "[5 rows x 1919 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Merge the dataframes\n",
    "'''\n",
    "def merge_melt(sales, calendar, price, save_path=None):\n",
    "    sales_train = pd.melt(sales, id_vars = ['id', 'item_id', 'dept_id',\n",
    "                                            'cat_id', 'store_id', 'state_id'],\n",
    "                          var_name = 'day',value_name = 'demand')\n",
    "    train_df = pd.merge(sales_train, calendar, how='left',\n",
    "                        left_on=['day'], right_on=['d'])\n",
    "    train_df = reduce_mem_usage(train_df)\n",
    "    train_df = pd.merge(train_df, price, how='left', \n",
    "                        on=['store_id','item_id','wm_yr_wk'])\n",
    "    if save_path:\n",
    "        train_df.to_csv(save_path, index=False)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 6953.16 Mb (32.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_df = merge_melt(sales, calendar, price, save_path='../m5-forecasting-accuracy/merged_train.csv')\n",
    "train_df = reduce_mem_usage(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMean + WK/M lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(train_df, out_path=None):\n",
    "    # weekday: overlap with wday\n",
    "    # wm_yr_wk: index for merging, no additional info\n",
    "    # date: no additional info\n",
    "    # d: overlap with day\n",
    "    drop_col = ['wm_yr_wk', 'date', 'd', 'weekday']\n",
    "    train_df.drop(drop_col, inplace=True, axis=1) \n",
    "    print('Start calculating week and month lag...')\n",
    "    # week and month shift\n",
    "    lags = list(range(1,8))+[28]\n",
    "    for l in lags:\n",
    "        train_df[f\"lag_{l}\"] = train_df[['id', 'demand']].groupby('id')['demand'].shift(l)\n",
    "\n",
    "    print('Start calculating rolling mean...')\n",
    "    # rolling mean\n",
    "    window = [7,28]\n",
    "    for w in window:\n",
    "        for l in window:\n",
    "            train_df[f\"rmean_{l}_{w}\"] = train_df[['id',f\"lag_{l}\"]].groupby('id')[f\"lag_{l}\"].transform(\n",
    "                                                    lambda x: x.rolling(w).mean())\n",
    "    if not out_path:\n",
    "        print('Saving final train_df...')\n",
    "        train_df.to_csv(out_path, index=False)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = feature_eng(train_df, out_path='../m5-forecasting-accuracy/merged_train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change nans to string type\n",
    "# perform label encoder\n",
    "def encoder(train_df, d=None, encoder_path=None):\n",
    "    nan_feature = [\"event_name_1\", \"event_name_2\", \"event_type_1\", \n",
    "                   \"event_type_2\"]\n",
    "    for f in nan_feature:\n",
    "        print(f\"converting {f}\")\n",
    "        train_df[f][pd.isna(train_df[f])] = 'NaN'\n",
    "\n",
    "\n",
    "    # convert category features to non-negative int\n",
    "    cat_feature = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id',\n",
    "                  \"event_name_1\", \"event_name_2\", \"event_type_1\", \n",
    "                   \"event_type_2\"]\n",
    "    \n",
    "    print('start label encoder...')\n",
    "    if not d:\n",
    "        d = defaultdict(LabelEncoder)\n",
    "        fit = train_df[cat_feature].apply(lambda x: d[x.name].fit_transform(x))\n",
    "        if encoder_path:\n",
    "            joblib.dump(d, encoder_path)\n",
    "    else:\n",
    "        fit = train_df[cat_feature].apply(lambda x: d[x.name].transform(x))\n",
    "    \n",
    "    print('finish label encoder')  \n",
    "    train_df = pd.concat([train_df[train_df.columns[~train_df.columns.isin(cat_feature)]],\n",
    "                        fit], axis=1)\n",
    "    # # Inverse the encoded\n",
    "    # fit.apply(lambda x: d[x.name].inverse_transform(x))\n",
    "\n",
    "    # # Using the dictionary to label future data\n",
    "    # df.apply(lambda x: d[x.name].transform(x))\n",
    "    return train_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start saving valid df ...\n",
      "start saving train df\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "def split_and_downsample(train_df, all_path, train_path, valid_path):\n",
    "    train_df['day_int'] = [int(i.split('_')[1]) for i in train_df['day']]\n",
    "    print('start saving full df ...')\n",
    "    train_df.to_csv(all_path, index=False)\n",
    "    # deleting the first few days that do not have lag and rolling mean\n",
    "    train_df = train_df[train_df['day_int']>=56]\n",
    "    valid_df = train_df[train_df['day_int']>=1885]\n",
    "    print('start saving valid df ...')\n",
    "    valid_df.to_csv(valid_path, index=False)\n",
    "    # small train data\n",
    "    # train_df = train_df[train_df['day_int']>=156]\n",
    "    train_df = train_df[train_df['day_int']<1885]\n",
    "    print('start saving train df')\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    return train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_df(sales, calendar, price, encoder_path, out_path):\n",
    "    test_sales = pd.concat([sales.iloc[:,:6],sales.iloc[:,-56:]], axis=1)\n",
    "    tmp = [np.nan]*test_sales.shape[0]\n",
    "    for i in range(28):\n",
    "        test_sales[f\"d_{1914+i}\"] = tmp\n",
    "    print('Melting df...')\n",
    "    test_df = merge_melt(pd.concat([test_sales.iloc[:,:6],test_sales.iloc[:,-28:]], axis=1),\n",
    "                         calendar, price)\n",
    "    print('Transform categorical feature')\n",
    "    b = joblib.load(encoder_path)\n",
    "    test_df = encoder(test_df, d=b)\n",
    "    print('Save df to csv file')\n",
    "    test_df.to_csv(out_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting event_name_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NicoleQi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting event_name_2\n",
      "converting event_type_1\n",
      "converting event_type_2\n",
      "start label encoder...\n",
      "finish label encoder\n"
     ]
    }
   ],
   "source": [
    "train_df = encoder(train_df, encoder_path='../m5-forecasting-accuracy/label_encoder_dict_new.joblib')\n",
    "\n",
    "train_df = split_and_downsample(train_df, '../m5-forecasting-accuracy/preprocessed_all.csv','../m5-forecasting-accuracy/preprocessed_train_new.csv', '../m5-forecasting-accuracy/preprocessed_valid_new.csv')\n",
    "\n",
    "create_test_df(sales, calendar, price, encoder_path='../m5-forecasting-accuracy/label_encoder_dict_new.joblib', out_path='../m5-forecasting-accuracy/preprocessed_test_new.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train lgbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params, in_file, out_file):\n",
    "    train_df = pd.read_csv(in_file, dtype=DTYPE)\n",
    "    print('Finish Loading')\n",
    "    drop_col = ['id', 'day', 'weekday','demand', 'day_int']\n",
    "    X_train = train_df[train_df.columns[~train_df.columns.isin(drop_col)]]\n",
    "    y_train = train_df['demand']\n",
    "\n",
    "    \n",
    "    np.random.seed(12)\n",
    "\n",
    "    cat_feature = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id',\n",
    "                  \"event_name_1\", \"event_name_2\", \"event_type_1\", \n",
    "                   \"event_type_2\", 'snap_CA', 'snap_TX', 'snap_WI']\n",
    "    valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\n",
    "    train_inds = np.setdiff1d(X_train.index.values, valid_inds)\n",
    "    train_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n",
    "                             categorical_feature=cat_feature, free_raw_data=False)\n",
    "\n",
    "    valid_data = lgb.Dataset(X_train.loc[valid_inds], label = y_train.loc[valid_inds],\n",
    "                            categorical_feature=cat_feature, free_raw_data=False)\n",
    "    print('Finish packing dataset')\n",
    "    del train_df, X_train, y_train, valid_inds,train_inds ; gc.collect()\n",
    "\n",
    "    m_lgb = lgb.train(params, train_data, valid_sets = [valid_data], verbose_eval=20) \n",
    "\n",
    "    m_lgb.save_model(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#         \"objective\" : \"poisson\",\n",
    "#         \"metric\" :\"rmse\",\n",
    "#         \"force_row_wise\" : True,\n",
    "#         \"learning_rate\" : 0.04,\n",
    "# #         \"sub_feature\" : 0.8,\n",
    "#         \"sub_row\" : 0.75,\n",
    "#         \"bagging_freq\" : 1,\n",
    "#         \"lambda_l2\" : 0.1,\n",
    "# #         \"nthread\" : 4\n",
    "#         \"metric\": [\"rmse\"],\n",
    "#     'verbosity': 1,\n",
    "#     'num_iterations' : 2000,\n",
    "#     'num_leaves': 128,\n",
    "#     \"min_data_in_leaf\": 100,\n",
    "#     \"early_stopping_round\": 5,\n",
    "# }\n",
    "\n",
    "\n",
    "params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "#                     'device_type':'gpu',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 63,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'nthread' : 4,\n",
    "                    'verbose': -1,\n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8min 22s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "train_df = pd.read_csv('../m5-forecasting-accuracy/preprocessed_train_new.csv')\n",
    "drop_col = ['id', 'day', 'weekday','demand', 'day_int']\n",
    "X_train = train_df[train_df.columns[~train_df.columns.isin(drop_col)]]\n",
    "y_train = train_df['demand']\n",
    "\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "cat_feature = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id',\n",
    "              \"event_name_1\", \"event_name_2\", \"event_type_1\", \n",
    "               \"event_type_2\", 'snap_CA', 'snap_TX', 'snap_WI']\n",
    "valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\n",
    "train_inds = np.setdiff1d(X_train.index.values, valid_inds)\n",
    "train_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n",
    "                         categorical_feature=cat_feature, free_raw_data=False)\n",
    "\n",
    "valid_data = lgb.Dataset(X_train.loc[valid_inds], label = y_train.loc[valid_inds],\n",
    "                        categorical_feature=cat_feature, free_raw_data=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Loading\n",
      "Finish packing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NicoleQi/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/NicoleQi/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tvalid_0's rmse: 3.12465\n",
      "[40]\tvalid_0's rmse: 2.51866\n",
      "[60]\tvalid_0's rmse: 2.251\n",
      "[80]\tvalid_0's rmse: 2.14849\n",
      "[100]\tvalid_0's rmse: 2.10617\n",
      "[120]\tvalid_0's rmse: 2.088\n",
      "[140]\tvalid_0's rmse: 2.0751\n",
      "[160]\tvalid_0's rmse: 2.06662\n",
      "[180]\tvalid_0's rmse: 2.05727\n",
      "[200]\tvalid_0's rmse: 2.05111\n",
      "[220]\tvalid_0's rmse: 2.04706\n",
      "[240]\tvalid_0's rmse: 2.04381\n",
      "[260]\tvalid_0's rmse: 2.04043\n",
      "[280]\tvalid_0's rmse: 2.0368\n",
      "[300]\tvalid_0's rmse: 2.03468\n",
      "[320]\tvalid_0's rmse: 2.03182\n",
      "[340]\tvalid_0's rmse: 2.03\n",
      "[360]\tvalid_0's rmse: 2.02822\n",
      "[380]\tvalid_0's rmse: 2.02689\n",
      "[400]\tvalid_0's rmse: 2.02558\n",
      "[420]\tvalid_0's rmse: 2.02421\n",
      "[440]\tvalid_0's rmse: 2.02325\n",
      "[460]\tvalid_0's rmse: 2.02201\n",
      "[480]\tvalid_0's rmse: 2.01998\n",
      "[500]\tvalid_0's rmse: 2.01888\n",
      "[520]\tvalid_0's rmse: 2.01747\n",
      "[540]\tvalid_0's rmse: 2.01608\n",
      "[560]\tvalid_0's rmse: 2.01508\n",
      "[580]\tvalid_0's rmse: 2.01387\n",
      "[600]\tvalid_0's rmse: 2.01239\n",
      "[620]\tvalid_0's rmse: 2.01153\n",
      "[640]\tvalid_0's rmse: 2.01022\n",
      "[660]\tvalid_0's rmse: 2.00958\n",
      "[680]\tvalid_0's rmse: 2.00906\n",
      "[700]\tvalid_0's rmse: 2.0079\n",
      "[720]\tvalid_0's rmse: 2.00716\n",
      "[740]\tvalid_0's rmse: 2.00613\n",
      "[760]\tvalid_0's rmse: 2.00532\n",
      "[780]\tvalid_0's rmse: 2.0047\n",
      "[800]\tvalid_0's rmse: 2.004\n",
      "[820]\tvalid_0's rmse: 2.00334\n",
      "[840]\tvalid_0's rmse: 2.00245\n",
      "[860]\tvalid_0's rmse: 2.00187\n",
      "[880]\tvalid_0's rmse: 2.00116\n",
      "[900]\tvalid_0's rmse: 2.00062\n",
      "[920]\tvalid_0's rmse: 2.0002\n",
      "[940]\tvalid_0's rmse: 1.99978\n",
      "[960]\tvalid_0's rmse: 1.99872\n",
      "[980]\tvalid_0's rmse: 1.99797\n",
      "[1000]\tvalid_0's rmse: 1.99745\n",
      "[1020]\tvalid_0's rmse: 1.997\n",
      "[1040]\tvalid_0's rmse: 1.99656\n",
      "[1060]\tvalid_0's rmse: 1.99613\n",
      "[1080]\tvalid_0's rmse: 1.99571\n",
      "[1100]\tvalid_0's rmse: 1.99524\n",
      "[1120]\tvalid_0's rmse: 1.99435\n",
      "[1140]\tvalid_0's rmse: 1.99403\n",
      "[1160]\tvalid_0's rmse: 1.9936\n",
      "[1180]\tvalid_0's rmse: 1.99322\n",
      "[1200]\tvalid_0's rmse: 1.99278\n",
      "[1220]\tvalid_0's rmse: 1.99236\n",
      "[1240]\tvalid_0's rmse: 1.99202\n",
      "[1260]\tvalid_0's rmse: 1.99147\n",
      "[1280]\tvalid_0's rmse: 1.99102\n",
      "[1300]\tvalid_0's rmse: 1.99087\n",
      "[1320]\tvalid_0's rmse: 1.99053\n",
      "[1340]\tvalid_0's rmse: 1.99028\n",
      "[1360]\tvalid_0's rmse: 1.99005\n"
     ]
    }
   ],
   "source": [
    "%lprun -f train train(params, '../m5-forecasting-accuracy/preprocessed_train_new.csv', '../m5-forecasting-accuracy/base_model_new.lgb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_path, sales, valid_df, start, out_path):\n",
    "    m_lgb = lgb.Booster(model_file= model_path)\n",
    "    if start==1886:\n",
    "        valid_sales = pd.concat([sales.iloc[:,:6],sales.iloc[:,-84:-28]], axis=1)\n",
    "    else:\n",
    "        valid_sales = pd.concat([sales.iloc[:,:6],sales.iloc[:,-56:]], axis=1)\n",
    "    tmp = [np.nan]*valid_sales.shape[0]\n",
    "    for i in range(28):\n",
    "        valid_sales[f\"d_{start+i}\"] = tmp\n",
    "    drop_col = ['id', 'day', 'date', 'wm_yr_wk', 'd', 'weekday','demand', 'day_int']\n",
    "    lags = list(range(1,8))+[28]\n",
    "    window = [7,28]\n",
    "    for i in range(28):\n",
    "        print(f\"start predicting d_{start+i}...\")\n",
    "        test = valid_df[valid_df['day']==f\"d_{start+i}\"].copy()\n",
    "        for l in lags:\n",
    "    #         print(test_sales.iloc[:, 62+i-l])\n",
    "            test[f\"lag_{l}\"] = valid_sales.iloc[:, 62+i-l].values\n",
    "        for l in window:\n",
    "            for w in window:\n",
    "                test[f\"rmean_{l}_{w}\"] = valid_sales.iloc[:, 62+i-l-w:62+i-l].mean(axis=1).values\n",
    "#         X_test = test[test.columns[~test.columns.isin(drop_col)]]\n",
    "        X_test = test[FEATURE]\n",
    "        y_pred = m_lgb.predict(X_test, num_iteration=m_lgb.best_iteration)\n",
    "        valid_sales[f\"d_{start+i}\"] = y_pred\n",
    "    submission_valid = pd.concat([valid_sales.iloc[:, 1], valid_sales.iloc[:,-28:]], axis=1)\n",
    "    submission_valid.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv('../m5-forecasting-accuracy/preprocessed_valid_new.csv', dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predicting d_1886...\n",
      "start predicting d_1887...\n",
      "start predicting d_1888...\n",
      "start predicting d_1889...\n",
      "start predicting d_1890...\n",
      "start predicting d_1891...\n",
      "start predicting d_1892...\n",
      "start predicting d_1893...\n",
      "start predicting d_1894...\n",
      "start predicting d_1895...\n",
      "start predicting d_1896...\n",
      "start predicting d_1897...\n",
      "start predicting d_1898...\n",
      "start predicting d_1899...\n",
      "start predicting d_1900...\n",
      "start predicting d_1901...\n",
      "start predicting d_1902...\n",
      "start predicting d_1903...\n",
      "start predicting d_1904...\n",
      "start predicting d_1905...\n",
      "start predicting d_1906...\n",
      "start predicting d_1907...\n",
      "start predicting d_1908...\n",
      "start predicting d_1909...\n",
      "start predicting d_1910...\n",
      "start predicting d_1911...\n",
      "start predicting d_1912...\n",
      "start predicting d_1913...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%lprun -f predict predict(\"../m5-forecasting-accuracy/base_model_new.lgb\", sales, valid_df, 1886, '../m5-forecasting-accuracy/submission_valid_new.csv')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 353.383 s\n",
    "File: <ipython-input-23-9e30aa80a158>\n",
    "Function: predict at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def predict(model_path, sales, valid_df, start, out_path):\n",
    "     2         1     499212.0 499212.0      0.1      m_lgb = lgb.Booster(model_file= model_path)\n",
    "     3         1          2.0      2.0      0.0      if start==1886:\n",
    "     4         1      12148.0  12148.0      0.0          valid_sales = pd.concat([sales.iloc[:,:6],sales.iloc[:,-84:-28]], axis=1)\n",
    "     5                                               else:\n",
    "     6                                                   valid_sales = pd.concat([sales.iloc[:,:6],sales.iloc[:,-56:]], axis=1)\n",
    "     7         1        157.0    157.0      0.0      tmp = [np.nan]*valid_sales.shape[0]\n",
    "     8        29         51.0      1.8      0.0      for i in range(28):\n",
    "     9        28     131083.0   4681.5      0.0          valid_sales[f\"d_{start+i}\"] = tmp\n",
    "    10         1          1.0      1.0      0.0      drop_col = ['id', 'day', 'date', 'wm_yr_wk', 'd', 'weekday','demand', 'day_int']\n",
    "    11         1          3.0      3.0      0.0      lags = list(range(1,8))+[28]\n",
    "    12         1          1.0      1.0      0.0      window = [7,28]\n",
    "    13        29         58.0      2.0      0.0      for i in range(28):\n",
    "    14        28       7470.0    266.8      0.0          print(f\"start predicting d_{start+i}...\")\n",
    "    15        28    2009068.0  71752.4      0.6          test = valid_df[valid_df['day']==f\"d_{start+i}\"].copy()\n",
    "    16       252        474.0      1.9      0.0          for l in lags:\n",
    "    17                                               #         print(test_sales.iloc[:, 62+i-l])\n",
    "    18       224     200561.0    895.4      0.1              test[f\"lag_{l}\"] = valid_sales.iloc[:, 62+i-l].values\n",
    "    19        84         88.0      1.0      0.0          for l in window:\n",
    "    20       168        292.0      1.7      0.0              for w in window:\n",
    "    21       112     679311.0   6065.3      0.2                  test[f\"rmean_{l}_{w}\"] = valid_sales.iloc[:, 62+i-l-w:62+i-l].mean(axis=1).values\n",
    "    22        28     278852.0   9959.0      0.1          X_test = test[test.columns[~test.columns.isin(drop_col)]]\n",
    "    23        28  348241204.0 12437185.9     98.5          y_pred = m_lgb.predict(X_test, num_iteration=m_lgb.best_iteration)\n",
    "    24        28      15111.0    539.7      0.0          valid_sales[f\"d_{start+i}\"] = y_pred\n",
    "    25         1      25741.0  25741.0      0.0      submission_valid = pd.concat([valid_sales.iloc[:, 1], valid_sales.iloc[:,-28:]], axis=1)\n",
    "    26         1    1281859.0 1281859.0      0.4      submission_valid.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predicting d_1914...\n",
      "start predicting d_1915...\n",
      "start predicting d_1916...\n",
      "start predicting d_1917...\n",
      "start predicting d_1918...\n",
      "start predicting d_1919...\n",
      "start predicting d_1920...\n",
      "start predicting d_1921...\n",
      "start predicting d_1922...\n",
      "start predicting d_1923...\n",
      "start predicting d_1924...\n",
      "start predicting d_1925...\n",
      "start predicting d_1926...\n",
      "start predicting d_1927...\n",
      "start predicting d_1928...\n",
      "start predicting d_1929...\n",
      "start predicting d_1930...\n",
      "start predicting d_1931...\n",
      "start predicting d_1932...\n",
      "start predicting d_1933...\n",
      "start predicting d_1934...\n",
      "start predicting d_1935...\n",
      "start predicting d_1936...\n",
      "start predicting d_1937...\n",
      "start predicting d_1938...\n",
      "start predicting d_1939...\n",
      "start predicting d_1940...\n",
      "start predicting d_1941...\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('../m5-forecasting-accuracy/preprocessed_test_new.csv', dtype=DTYPE)\n",
    "predict(\"../m5-forecasting-accuracy/base_model_850.lgb\", sales, test_df, 1914, '../m5-forecasting-accuracy/submission_test_new.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
