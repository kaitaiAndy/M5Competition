{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 Forecasting\n",
    "\n",
    "[Introduction](#Introduction)\n",
    "\n",
    "\n",
    "[EDA](#EDA)\n",
    "\n",
    "To-do\n",
    "- Denoising\n",
    "\n",
    "Statistical Model\n",
    "- ARIMA\n",
    "- Exponential Smoothing\n",
    "- Theta Method\n",
    "\n",
    "Machine Learning Model\n",
    "- GBM\n",
    "- LSTM\n",
    "Multi-step ahead forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Goal\n",
    "Predict Sales data provided by Walmart **28** days into the future\n",
    "\n",
    "## Data\n",
    "sales_train.csv: this is our main training data. It has 1 column for each of the 1941 days from 2011-01-29 and 2016-05-22; not including the validation period of 28 days until 2016-06-19. It also includes the IDs for item, department, category, store, and state. The number of rows is 30490 for all combinations of 30490 items and 10 stores.\n",
    "\n",
    "sell_prices.csv: the store and item IDs together with the sales price of the item as a weekly average.\n",
    "\n",
    "calendar.csv: dates together with related features like day-of-the week, month, year, and an 3 binary flags for whether the stores in each state allowed purchases with SNAP food stamps at this date (1) or not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import joblib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "util function\n",
    "https://www.kaggle.com/ratan123/m5-forecasting-lightgbm-with-timeseries-splits\n",
    "'''\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_name, sales_path='sales_train_validation.csv',\n",
    "             calendar_path='calendar.csv', price_path='sell_prices.csv'):\n",
    "    sales = pd.read_csv(os.path.join(dir_name, sales_path))\n",
    "    calendar = pd.read_csv(os.path.join(dir_name, calendar_path))\n",
    "    price = pd.read_csv(os.path.join(dir_name, price_path))\n",
    "    return sales, calendar, price\n",
    "#     return reduce_mem_usage(sales), reduce_mem_usage(calendar), reduce_mem_usage(price)\n",
    "\n",
    "sales, calendar, price = load_data('../m5-forecasting-accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1904</th>\n",
       "      <th>d_1905</th>\n",
       "      <th>d_1906</th>\n",
       "      <th>d_1907</th>\n",
       "      <th>d_1908</th>\n",
       "      <th>d_1909</th>\n",
       "      <th>d_1910</th>\n",
       "      <th>d_1911</th>\n",
       "      <th>d_1912</th>\n",
       "      <th>d_1913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1904  d_1905  d_1906  d_1907  d_1908  \\\n",
       "0       CA    0    0    0    0  ...       1       3       0       1       1   \n",
       "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
       "2       CA    0    0    0    0  ...       2       1       2       1       1   \n",
       "3       CA    0    0    0    0  ...       1       0       5       4       1   \n",
       "4       CA    0    0    0    0  ...       2       1       1       0       1   \n",
       "\n",
       "   d_1909  d_1910  d_1911  d_1912  d_1913  \n",
       "0       1       3       0       1       1  \n",
       "1       1       0       0       0       0  \n",
       "2       1       0       1       1       1  \n",
       "3       0       1       3       7       2  \n",
       "4       1       2       2       2       4  \n",
       "\n",
       "[5 rows x 1919 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Merge the dataframes\n",
    "'''\n",
    "def merge_melt(sales, calendar, price, save_path=None):\n",
    "    sales_train = pd.melt(sales, id_vars = ['id', 'item_id', 'dept_id',\n",
    "                                            'cat_id', 'store_id', 'state_id'],\n",
    "                          var_name = 'day',value_name = 'demand')\n",
    "    train_df = pd.merge(sales_train, calendar, how='left',\n",
    "                        left_on=['day'], right_on=['d'])\n",
    "    train_df = reduce_mem_usage(train_df)\n",
    "    train_df = pd.merge(train_df, price, how='left', \n",
    "                        on=['store_id','item_id','wm_yr_wk'])\n",
    "    if save_path:\n",
    "        train_df.to_csv(save_path, index=False)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 6953.16 Mb (32.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_df = merge_melt(sales, calendar, price, save_path='../m5-forecasting-accuracy/merged_train.csv')\n",
    "train_df = reduce_mem_usage(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMean + WK/M lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekday: overlap with wday\n",
    "# wm_yr_wk: index for merging, no additional info\n",
    "# date: no additional info\n",
    "# d: overlap with day\n",
    "drop_col = ['wm_yr_wk', 'date', 'd', 'weekday']\n",
    "train_df.drop(drop_col, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58327370, 19)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week and month shift\n",
    "lags = list(range(1,8))+[28]\n",
    "for l in lags:\n",
    "    train_df[f\"lag_{l}\"] = train_df[['id', 'demand']].groupby('id')['demand'].shift(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling mean\n",
    "window = [7,28]\n",
    "for w in window:\n",
    "    for l in window:\n",
    "        train_df[f\"rmean_{l}_{w}\"] = train_df[['id',f\"lag_{l}\"]].groupby('id')[f\"lag_{l}\"].transform(\n",
    "                                                lambda x: x.rolling(w).mean())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../m5-forecasting-accuracy/merged_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NicoleQi/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (12,13,14,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading dataframe\n",
      "starting reducing mem use\n",
      "Mem. usage decreased to 7286.92 Mb (48.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../m5-forecasting-accuracy/merged_train.csv')\n",
    "print('finish loading dataframe')\n",
    "print('starting reducing mem use')\n",
    "train_df = reduce_mem_usage(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     0\n",
       "item_id                0\n",
       "dept_id                0\n",
       "cat_id                 0\n",
       "store_id               0\n",
       "state_id               0\n",
       "day                    0\n",
       "demand                 0\n",
       "weekday                0\n",
       "wday                   0\n",
       "month                  0\n",
       "year                   0\n",
       "event_name_1    53631910\n",
       "event_type_1    53631910\n",
       "event_name_2    58205410\n",
       "event_type_2    58205410\n",
       "snap_CA                0\n",
       "snap_TX                0\n",
       "snap_WI                0\n",
       "sell_price      12299413\n",
       "lag_1              30499\n",
       "lag_2              60989\n",
       "lag_3              91479\n",
       "lag_4             121969\n",
       "lag_5             152459\n",
       "lag_6             182949\n",
       "lag_7             213439\n",
       "lag_28            853729\n",
       "rmean_7_7         396379\n",
       "rmean_28_7       1036669\n",
       "rmean_7_28       1036669\n",
       "rmean_28_28      1676959\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_na = pd.isna(train_df)\n",
    "check_na.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change nans to string type\n",
    "# perform label encoder\n",
    "def encoder(train_df, d=None, encoder_path=None):\n",
    "    nan_feature = [\"event_name_1\", \"event_name_2\", \"event_type_1\", \n",
    "                   \"event_type_2\"]\n",
    "    for f in nan_feature:\n",
    "        print(f\"converting {f}\")\n",
    "        train_df[f][pd.isna(train_df[f])] = 'NaN'\n",
    "\n",
    "\n",
    "    # convert category features to non-negative int\n",
    "    cat_feature = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id',\n",
    "                  \"event_name_1\", \"event_name_2\", \"event_type_1\", \n",
    "                   \"event_type_2\"]\n",
    "    \n",
    "    print('start label encoder...')\n",
    "    if not d:\n",
    "        d = defaultdict(LabelEncoder)\n",
    "        fit = train_df[cat_feature].apply(lambda x: d[x.name].fit_transform(x))\n",
    "        if encoder_path:\n",
    "            joblib.dump(d, encoder_path)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        fit = train_df[cat_feature].apply(lambda x: d[x.name].transform(x))\n",
    "    \n",
    "    print('finish label encoder')  \n",
    "    train_df = pd.concat([train_df[train_df.columns[~train_df.columns.isin(cat_feature)]],\n",
    "                        fit], axis=1)\n",
    "    # # Inverse the encoded\n",
    "    # fit.apply(lambda x: d[x.name].inverse_transform(x))\n",
    "\n",
    "    # # Using the dictionary to label future data\n",
    "    # df.apply(lambda x: d[x.name].transform(x))\n",
    "    return train_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../m5-forecasting-accuracy/label_encoder_dict.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = encoder(train_df, '../m5-forecasting-accuracy/label_encoder_dict.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               object\n",
       "day              object\n",
       "demand            int16\n",
       "weekday          object\n",
       "wday               int8\n",
       "month              int8\n",
       "year              int16\n",
       "snap_CA            int8\n",
       "snap_TX            int8\n",
       "snap_WI            int8\n",
       "sell_price      float16\n",
       "lag_1           float16\n",
       "lag_2           float16\n",
       "lag_3           float16\n",
       "lag_4           float16\n",
       "lag_5           float16\n",
       "lag_6           float16\n",
       "lag_7           float16\n",
       "lag_28          float16\n",
       "rmean_7_7       float16\n",
       "rmean_28_7      float16\n",
       "rmean_7_28      float16\n",
       "rmean_28_28     float16\n",
       "item_id           int64\n",
       "dept_id           int64\n",
       "store_id          int64\n",
       "cat_id            int64\n",
       "state_id          int64\n",
       "event_name_1      int64\n",
       "event_name_2      int64\n",
       "event_type_1      int64\n",
       "event_type_2      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../m5-forecasting-accuracy/preprocessed_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train lgbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               object\n",
       "day              object\n",
       "demand            int64\n",
       "weekday          object\n",
       "wday              int64\n",
       "month             int64\n",
       "year              int64\n",
       "snap_CA           int64\n",
       "snap_TX           int64\n",
       "snap_WI           int64\n",
       "sell_price      float64\n",
       "lag_1           float64\n",
       "lag_2           float64\n",
       "lag_3           float64\n",
       "lag_4           float64\n",
       "lag_5           float64\n",
       "lag_6           float64\n",
       "lag_7           float64\n",
       "lag_28          float64\n",
       "rmean_7_7       float64\n",
       "rmean_28_7      float64\n",
       "rmean_7_28      float64\n",
       "rmean_28_28     float64\n",
       "item_id           int64\n",
       "dept_id           int64\n",
       "store_id          int64\n",
       "cat_id            int64\n",
       "state_id          int64\n",
       "event_name_1      int64\n",
       "event_name_2      int64\n",
       "event_type_1      int64\n",
       "event_type_2      int64\n",
       "day_int           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../m5-forecasting-accuracy/preprocessed_all.csv')\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 3949.40 Mb (73.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_df = reduce_mem_usage(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start saving full df ...\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "\n",
    "train_df['day_int'] = [int(i.split('_')[1]) for i in train_df['day']]\n",
    "print('start saving full df ...')\n",
    "train_df.to_csv('../m5-forecasting-accuracy/preprocessed_all.csv', index=False)\n",
    "# train_df = train_df[train_df['day_int']>=56]\n",
    "# valid_df = train_df[train_df['day_int']>=1885]\n",
    "# print('start saving valid df ...')\n",
    "# valid_df.to_csv('../m5-forecasting-accuracy/preprocessed_valid.csv', index=False)\n",
    "# train_df = train_df[train_df['day_int']<1885]\n",
    "# print('start saving train df')\n",
    "# train_df.to_csv('../m5-forecasting-accuracy/preprocessed_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['day_int']>=156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = ['id', 'day', 'weekday','demand', 'day_int']\n",
    "X_train = train_df[train_df.columns[~train_df.columns.isin(drop_col)]]\n",
    "y_train = train_df['demand']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "cat_feature = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id',\n",
    "              \"event_name_1\", \"event_name_2\", \"event_type_1\", \n",
    "               \"event_type_2\", 'snap_CA', 'snap_TX', 'snap_WI']\n",
    "valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\n",
    "train_inds = np.setdiff1d(X_train.index.values, valid_inds)\n",
    "train_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n",
    "                         categorical_feature=cat_feature, free_raw_data=False)\n",
    "\n",
    "valid_data = lgb.Dataset(X_train.loc[valid_inds], label = y_train.loc[valid_inds],\n",
    "                        categorical_feature=cat_feature, free_raw_data=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_df, X_train, y_train, valid_inds,train_inds ; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"objective\" : \"poisson\",\n",
    "        \"metric\" :\"rmse\",\n",
    "        \"force_row_wise\" : True,\n",
    "        \"learning_rate\" : 0.03,\n",
    "#         \"sub_feature\" : 0.8,\n",
    "        \"sub_row\" : 0.75,\n",
    "        \"bagging_freq\" : 1,\n",
    "        \"lambda_l2\" : 0.1,\n",
    "#         \"nthread\" : 4\n",
    "        \"metric\": [\"rmse\"],\n",
    "    'verbosity': 1,\n",
    "    'num_iterations' : 1200,\n",
    "    'num_leaves': 128,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"early_stopping_round\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NicoleQi/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/NicoleQi/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/NicoleQi/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[20]\tvalid_0's rmse: 3.18851\n",
      "[40]\tvalid_0's rmse: 2.7408\n",
      "[60]\tvalid_0's rmse: 2.46126\n",
      "[80]\tvalid_0's rmse: 2.2922\n",
      "[100]\tvalid_0's rmse: 2.19249\n",
      "[120]\tvalid_0's rmse: 2.13459\n",
      "[140]\tvalid_0's rmse: 2.10064\n",
      "[160]\tvalid_0's rmse: 2.08069\n",
      "[180]\tvalid_0's rmse: 2.0687\n",
      "[200]\tvalid_0's rmse: 2.0611\n",
      "[220]\tvalid_0's rmse: 2.05577\n",
      "[240]\tvalid_0's rmse: 2.05202\n",
      "[260]\tvalid_0's rmse: 2.04898\n",
      "[280]\tvalid_0's rmse: 2.04612\n",
      "[300]\tvalid_0's rmse: 2.04436\n",
      "[320]\tvalid_0's rmse: 2.04238\n",
      "[340]\tvalid_0's rmse: 2.04072\n",
      "[360]\tvalid_0's rmse: 2.03925\n",
      "[380]\tvalid_0's rmse: 2.03674\n",
      "[400]\tvalid_0's rmse: 2.03452\n",
      "[420]\tvalid_0's rmse: 2.03257\n",
      "[440]\tvalid_0's rmse: 2.03064\n",
      "[460]\tvalid_0's rmse: 2.02903\n",
      "[480]\tvalid_0's rmse: 2.02736\n",
      "[500]\tvalid_0's rmse: 2.02537\n",
      "[520]\tvalid_0's rmse: 2.02339\n",
      "[540]\tvalid_0's rmse: 2.021\n",
      "[560]\tvalid_0's rmse: 2.01963\n",
      "[580]\tvalid_0's rmse: 2.01776\n",
      "[600]\tvalid_0's rmse: 2.01628\n",
      "[620]\tvalid_0's rmse: 2.0148\n",
      "[640]\tvalid_0's rmse: 2.01361\n",
      "[660]\tvalid_0's rmse: 2.01218\n",
      "[680]\tvalid_0's rmse: 2.01145\n",
      "[700]\tvalid_0's rmse: 2.00954\n",
      "[720]\tvalid_0's rmse: 2.00814\n",
      "[740]\tvalid_0's rmse: 2.0068\n",
      "[760]\tvalid_0's rmse: 2.00561\n",
      "[780]\tvalid_0's rmse: 2.00435\n",
      "[800]\tvalid_0's rmse: 2.00316\n",
      "[820]\tvalid_0's rmse: 2.00247\n",
      "[840]\tvalid_0's rmse: 2.00123\n",
      "[860]\tvalid_0's rmse: 1.99983\n",
      "[880]\tvalid_0's rmse: 1.99871\n",
      "[900]\tvalid_0's rmse: 1.99772\n",
      "[920]\tvalid_0's rmse: 1.99692\n",
      "[940]\tvalid_0's rmse: 1.99576\n",
      "[960]\tvalid_0's rmse: 1.99434\n",
      "[980]\tvalid_0's rmse: 1.99366\n",
      "[1000]\tvalid_0's rmse: 1.99287\n",
      "[1020]\tvalid_0's rmse: 1.99255\n",
      "[1040]\tvalid_0's rmse: 1.99208\n",
      "[1060]\tvalid_0's rmse: 1.99124\n",
      "[1080]\tvalid_0's rmse: 1.99041\n",
      "[1100]\tvalid_0's rmse: 1.98964\n",
      "[1120]\tvalid_0's rmse: 1.98889\n",
      "[1140]\tvalid_0's rmse: 1.98827\n",
      "[1160]\tvalid_0's rmse: 1.98745\n",
      "[1180]\tvalid_0's rmse: 1.98702\n",
      "[1200]\tvalid_0's rmse: 1.98614\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\tvalid_0's rmse: 1.98614\n"
     ]
    }
   ],
   "source": [
    "m_lgb = lgb.train(params, train_data, valid_sets = [valid_data], verbose_eval=20) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x113132080>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_lgb.save_model(\"../m5-forecasting-accuracy/base_model.lgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sales = pd.concat([sales.iloc[:,:6],sales.iloc[:,-56:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [np.nan]*test_sales.shape[0]\n",
    "for i in range(28):\n",
    "    test_sales[f\"d_{1914+i}\"] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 111.54 Mb (25.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "test_df = merge_melt(pd.concat([test_sales.iloc[:,:6],test_sales.iloc[:,-28:]], axis=1),\n",
    "                     calendar, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting event_name_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NicoleQi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting event_name_2\n",
      "converting event_type_1\n",
      "converting event_type_2\n",
      "start label encoder...\n",
      "finish label encoder\n"
     ]
    }
   ],
   "source": [
    "test_df = encoder(test_df, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               object\n",
       "day              object\n",
       "demand          float64\n",
       "date             object\n",
       "wm_yr_wk          int16\n",
       "weekday          object\n",
       "wday               int8\n",
       "month              int8\n",
       "year              int16\n",
       "d                object\n",
       "snap_CA            int8\n",
       "snap_TX            int8\n",
       "snap_WI            int8\n",
       "sell_price      float64\n",
       "item_id           int64\n",
       "dept_id           int64\n",
       "store_id          int64\n",
       "cat_id            int64\n",
       "state_id          int64\n",
       "event_name_1      int64\n",
       "event_name_2      int64\n",
       "event_type_1      int64\n",
       "event_type_2      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predicting d_1914...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NicoleQi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/NicoleQi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predicting d_1915...\n"
     ]
    }
   ],
   "source": [
    "drop_col = ['id', 'day', 'date', 'wm_yr_wk', 'd', 'weekday','demand', 'day_int']\n",
    "lags = list(range(1,8))+[28]\n",
    "window = [7,28]\n",
    "for i in range(28):\n",
    "    print(f\"start predicting d_{1914+i}...\")\n",
    "    test = test_df[test_df['d']==f\"d_{1914+i}\"]\n",
    "    for l in lags:\n",
    "#         print(test_sales.iloc[:, 62+i-l])\n",
    "        test[f\"lag_{l}\"] = test_sales.iloc[:, 62+i-l]\n",
    "    for l in window:\n",
    "        for w in window:\n",
    "            test[f\"rmean_{l}_{w}\"] = test_sales.iloc[:, 62+i-l-w:62+i-l].mean(axis=1)\n",
    "    X_test = test[test.columns[~test.columns.isin(drop_col)]]\n",
    "    y_pred = m_lgb.predict(X_test, num_iteration=m_lgb.best_iteration)\n",
    "    test_sales[f\"d_{1914+i}\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
